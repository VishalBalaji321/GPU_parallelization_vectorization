{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Basic Vectorization\n",
    "\n",
    "Vectorization refers to processing multiple compute-intensive steps in parallel. This is done by formulating by computation steps in terms of vectors and using efficient math like dot-product to speed up calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np # version = 1.24.3\n",
    "import torch # version = 2.0.0 + cu118"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing speedup for loop-based vs matmul based dot-product for 1D array\n",
    "- Two random 1D torch FloatTensors a, b are created each of size 10, 100, 1000, 10000, 100000\n",
    "- Time required to obtain dot product using loop is calculated as mean of 1000 runs to reduce variance\n",
    "- The same procedure is done for torch.matmul operation -> a @ b.T\n",
    "- Speedup is represented as multiples: Ratio of ```time_taken_for_loop / time_taken_for_matmul```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a, b: 10| Dot Product| Value: 3.12E+00, Loop time: 9.32E-05s\n",
      "Size of a, b: 10| Dot Product| Value: 3.12E+00, Matmul time: 4.37E-06s\n",
      "SpeedUp of matmul compared to loop for size 10: 19.1 x\n",
      "\n",
      "Size of a, b: 100| Dot Product| Value: 2.40E+01, Loop time: 7.23E-04s\n",
      "Size of a, b: 100| Dot Product| Value: 2.40E+01, Matmul time: 3.39E-06s\n",
      "SpeedUp of matmul compared to loop for size 100: 214.33 x\n",
      "\n",
      "Size of a, b: 1000| Dot Product| Value: 2.44E+02, Loop time: 7.20E-03s\n",
      "Size of a, b: 1000| Dot Product| Value: 2.44E+02, Matmul time: 3.61E-06s\n",
      "SpeedUp of matmul compared to loop for size 1000: 2019.48 x\n",
      "\n",
      "Size of a, b: 10000| Dot Product| Value: 2.51E+03, Loop time: 7.18E-02s\n",
      "Size of a, b: 10000| Dot Product| Value: 2.51E+03, Matmul time: 4.54E-06s\n",
      "SpeedUp of matmul compared to loop for size 10000: 16220.42 x\n",
      "\n",
      "Size of a, b: 100000| Dot Product| Value: 2.49E+04, Loop time: 7.12E-01s\n",
      "Size of a, b: 100000| Dot Product| Value: 2.49E+04, Matmul time: 7.79E-06s\n",
      "SpeedUp of matmul compared to loop for size 100000: 93815.77 x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SIZES_OF_ARRAY = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "for size_of_array in SIZES_OF_ARRAY:\n",
    "    a = torch.rand(size_of_array)\n",
    "    b = torch.rand(size_of_array)\n",
    "\n",
    "    avg_loop_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        dot_loop_value = 0.0\n",
    "        t_loop_start = perf_counter()\n",
    "        for index in range(a.shape[0]):\n",
    "            dot_loop_value += a[index] * b[index]\n",
    "        t_loop_end = perf_counter()\n",
    "        \n",
    "        t_loop_total = t_loop_end - t_loop_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_loop_time.append(t_loop_total)\n",
    "    avg_loop_time = torch.mean(torch.FloatTensor(avg_loop_time))\n",
    "    print(f\"Size of a, b: {size_of_array}| Dot Product| Value: {dot_loop_value:.2E}, Loop time: {avg_loop_time:.2E}s\")\n",
    "\n",
    "    avg_matmul_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        t_matmul_start = perf_counter()\n",
    "        dot_np_value = a @ b.T\n",
    "        t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_matmul_time.append(t_matmul_total)\n",
    "    avg_matmul_time = torch.mean(torch.FloatTensor(avg_matmul_time))\n",
    "    print(f\"Size of a, b: {size_of_array}| Dot Product| Value: {dot_np_value:.2E}, Matmul time: {avg_matmul_time:.2E}s\")\n",
    "\n",
    "    speedUp = (avg_loop_time / avg_matmul_time)\n",
    "    print(f\"SpeedUp of matmul compared to loop for size {size_of_array}: {round(speedUp, 2)} x\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing speedup for loop-based vs torch.matmul for matrix multiplication of N-D array\n",
    "- Two random N-D Torch FloatTensors a, b are created each of size 10, 100, 1000, 10000, 100000\n",
    "- Time required to obtain dot product using loop is calculated as mean of 20 runs to reduce variance\n",
    "- The same procedure is done for torch.matmul operation -> a @ b\n",
    "- Speedup is represented as multiples: Ratio of ```time_taken_for_loop / time_taken_for_matmul```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a, b: 10| Loop time: 2.65E-02s\n",
      "Size of a, b: 10| torch.matmul time: 6.70E-06s\n",
      "SpeedUp of matmul for N-D tensor compared to loop for size 10: 3955.0 x\n",
      "\n",
      "Size of a, b: 100| Loop time: 2.77E+01s\n",
      "Size of a, b: 100| torch.matmul time: 7.45E-05s\n",
      "SpeedUp of matmul for N-D tensor compared to loop for size 100: 372223.98 x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SIZES_OF_ARRAY = [10, 100] # , 1000, 10000]\n",
    "avg_times_matmul = []\n",
    "for size_of_array in SIZES_OF_ARRAY:\n",
    "    a = torch.rand((size_of_array, size_of_array))\n",
    "    b = torch.rand((size_of_array, size_of_array))\n",
    "\n",
    "    avg_loop_time = []\n",
    "    for _ in range(5): # Doing same computation for 20 times to reduce runtime variance\n",
    "        result = []\n",
    "        t_loop_start = perf_counter()\n",
    "        for i in range(a.shape[0]):\n",
    "            row = []\n",
    "            for j in range(b.shape[1]):\n",
    "                product = 0\n",
    "                for k in range(a.shape[1]):\n",
    "                    product += a[i][k] * b[k][j]\n",
    "                row.append(product)\n",
    "            result.append(row)\n",
    "        \n",
    "        t_loop_end = perf_counter()\n",
    "        \n",
    "        t_loop_total = t_loop_end - t_loop_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_loop_time.append(t_loop_total)\n",
    "    avg_loop_time = np.mean(avg_loop_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| Loop time: {avg_loop_time:.2E}s\")\n",
    "\n",
    "    avg_matmul_time = []\n",
    "    for _ in range(5): # Doing same computation for 20 times to reduce runtime variance\n",
    "        t_matmul_start = perf_counter()\n",
    "        dot_np_value = a @ b\n",
    "        t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_matmul_time.append(t_matmul_total)\n",
    "    avg_matmul_time = np.mean(avg_matmul_time)\n",
    "    avg_times_matmul.append(avg_matmul_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| torch.matmul time: {avg_matmul_time:.2E}s\")\n",
    "\n",
    "    speedUp = (avg_loop_time / avg_matmul_time)\n",
    "    print(f\"SpeedUp of matmul for N-D tensor compared to loop for size {size_of_array}: {round(speedUp, 2)} x\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.matmul for matrix multiplication of N-D array in GPUs/CPUs w.r.t Mixed Precision (Autocast)\n",
    "- Two random N-D Torch FloatTensors a, b are created each of size 10, 100, 1000, 10000, 100000\n",
    "- Time required to obtain dot product using loop is calculated as mean of 20 runs to reduce variance\n",
    "- The same procedure is done for torch.matmul operation -> a @ b\n",
    "- Speedup is represented as multiples: Ratio of ```time_taken_for_loop / time_taken_for_matmul```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a, b: 10| torch.matmul (GPU) time: 3.60E-05s\n",
      "Size of a, b: 10| torch.matmul (GPU + autocast) time: 9.73E-05s\n",
      "SpeedUp of matmul for AMP in GPU vs GPU for size 10: 0.37 x\n",
      "\n",
      "Size of a, b: 100| torch.matmul (GPU) time: 7.17E-05s\n",
      "Size of a, b: 100| torch.matmul (GPU + autocast) time: 5.05E-05s\n",
      "SpeedUp of matmul for AMP in GPU vs GPU for size 100: 1.42 x\n",
      "\n",
      "Size of a, b: 1000| torch.matmul (GPU) time: 4.10E-04s\n",
      "Size of a, b: 1000| torch.matmul (GPU + autocast) time: 2.70E-04s\n",
      "SpeedUp of matmul for AMP in GPU vs GPU for size 1000: 1.52 x\n",
      "\n",
      "Size of a, b: 10000| torch.matmul (GPU) time: 3.24E-01s\n",
      "Size of a, b: 10000| torch.matmul (GPU + autocast) time: 9.39E-02s\n",
      "SpeedUp of matmul for AMP in GPU vs GPU for size 10000: 3.44 x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SIZES_OF_ARRAY = [10, 100, 1000, 10000]\n",
    "gpu_device = torch.device(\"cuda\")\n",
    "\n",
    "for size_of_array in SIZES_OF_ARRAY:\n",
    "    a = torch.rand((size_of_array, size_of_array)).to(gpu_device)\n",
    "    b = torch.rand((size_of_array, size_of_array)).to(gpu_device)\n",
    "    torch.cuda.synchronize()     # Synchronizing to finish Async GPU operations\n",
    "\n",
    "    avg_gpu_matmul_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        t_matmul_start = perf_counter()\n",
    "        dot_value = a @ b # dot_np_value is float32\n",
    "        torch.cuda.synchronize() # Synchronizing to finish Async GPU operations\n",
    "        t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_gpu_matmul_time.append(t_matmul_total)\n",
    "    avg_gpu_matmul_time = np.mean(avg_gpu_matmul_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| torch.matmul (GPU) time: {avg_gpu_matmul_time:.2E}s\")\n",
    "\n",
    "    a = torch.rand((size_of_array, size_of_array)).to(device=gpu_device, dtype=torch.float16)\n",
    "    b = torch.rand((size_of_array, size_of_array)).to(device=gpu_device, dtype=torch.float16)\n",
    "    torch.cuda.synchronize()     # Synchronizing to finish Async GPU operations\n",
    "\n",
    "    avg_gpu_autocast_matmul_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            t_matmul_start = perf_counter()\n",
    "            dot_amp_value = a @ b # Due to autocast, dot_np_value type is float16, input dtypes are float32\n",
    "            torch.cuda.synchronize() # Synchronizing to finish Async GPU operations\n",
    "            t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_gpu_autocast_matmul_time.append(t_matmul_total)\n",
    "    avg_gpu_autocast_matmul_time = np.mean(avg_gpu_autocast_matmul_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| torch.matmul (GPU + autocast) time: {avg_gpu_autocast_matmul_time:.2E}s\")\n",
    "\n",
    "    speedUp = (avg_gpu_matmul_time / avg_gpu_autocast_matmul_time)\n",
    "    print(f\"SpeedUp of matmul for AMP in GPU vs GPU for size {size_of_array}: {round(speedUp, 2)} x\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of a, b: 10| torch.matmul (CPU) time: 2.83E-06s\n",
      "Size of a, b: 10| torch.matmul (CPU + autocast) time: 1.27E-05s\n",
      "SpeedUp of matmul for CPU vs AMP in CPU for size 10: 4.49 x\n",
      "\n",
      "Size of a, b: 100| torch.matmul (CPU) time: 3.84E-05s\n",
      "Size of a, b: 100| torch.matmul (CPU + autocast) time: 1.07E-02s\n",
      "SpeedUp of matmul for CPU vs AMP in CPU for size 100: 277.79 x\n",
      "\n",
      "Size of a, b: 1000| torch.matmul (CPU) time: 1.87E-02s\n",
      "Size of a, b: 1000| torch.matmul (CPU + autocast) time: 1.03E+01s\n",
      "SpeedUp of matmul for CPU vs AMP in CPU for size 1000: 548.9 x\n",
      "\n",
      "Size of a, b: 10000| torch.matmul (CPU) time: 1.13E+01s\n"
     ]
    }
   ],
   "source": [
    "SIZES_OF_ARRAY = [10, 100, 1000, 10000]\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "for size_of_array in SIZES_OF_ARRAY:\n",
    "    a = torch.rand((size_of_array, size_of_array)).to(cpu_device)\n",
    "    b = torch.rand((size_of_array, size_of_array)).to(cpu_device)\n",
    "    torch.cuda.synchronize()     # Synchronizing to finish Async GPU operations\n",
    "\n",
    "    avg_cpu_matmul_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        t_matmul_start = perf_counter()\n",
    "        dot_value = a @ b # dot_np_value is float32\n",
    "        t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_cpu_matmul_time.append(t_matmul_total)\n",
    "    avg_cpu_matmul_time = np.mean(avg_cpu_matmul_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| torch.matmul (CPU) time: {avg_cpu_matmul_time:.2E}s\")\n",
    "\n",
    "    a = torch.rand((size_of_array, size_of_array)).to(device=cpu_device, dtype=torch.bfloat16)\n",
    "    b = torch.rand((size_of_array, size_of_array)).to(device=cpu_device, dtype=torch.bfloat16)\n",
    "    torch.cuda.synchronize()     # Synchronizing to finish Async GPU operations\n",
    "\n",
    "    avg_cpu_autocast_matmul_time = []\n",
    "    for _ in range(20): # Doing same computation for 20 times to reduce runtime variance\n",
    "        with torch.autocast(\"cpu\"):\n",
    "            t_matmul_start = perf_counter()\n",
    "            dot_amp_value = a @ b # Due to autocast, dot_np_value type is float16, input dtypes are float32\n",
    "            t_matmul_end = perf_counter()\n",
    "        \n",
    "        t_matmul_total = t_matmul_end - t_matmul_start\n",
    "        if _ > 3: # Ignoring first 3 loop iterations, to further reduce the noise\n",
    "            avg_cpu_autocast_matmul_time.append(t_matmul_total)\n",
    "    avg_cpu_autocast_matmul_time = np.mean(avg_cpu_autocast_matmul_time)\n",
    "    print(f\"Size of a, b: {size_of_array}| torch.matmul (CPU + autocast) time: {avg_cpu_autocast_matmul_time:.2E}s\")\n",
    "\n",
    "    speedUp = (avg_cpu_autocast_matmul_time/ avg_cpu_matmul_time)\n",
    "    print(f\"SpeedUp of matmul for CPU vs AMP in CPU for size {size_of_array}: {round(speedUp, 2)} x\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
